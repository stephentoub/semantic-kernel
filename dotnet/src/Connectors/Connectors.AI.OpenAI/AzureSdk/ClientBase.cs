// Copyright (c) Microsoft. All rights reserved.

using System;
using System.Collections.Generic;
using System.Diagnostics.Metrics;
using System.Linq;
using System.Runtime.CompilerServices;
using System.Text;
using System.Threading;
using System.Threading.Tasks;
using Azure;
using Azure.AI.OpenAI;
using Microsoft.Extensions.Logging;
using Microsoft.Extensions.Logging.Abstractions;
using Microsoft.SemanticKernel.AI;
using Microsoft.SemanticKernel.AI.ChatCompletion;
using Microsoft.SemanticKernel.AI.TextCompletion;
using Microsoft.SemanticKernel.Connectors.AI.OpenAI.ChatCompletion;
using Microsoft.SemanticKernel.Diagnostics;
using Microsoft.SemanticKernel.Orchestration;
using Microsoft.SemanticKernel.Text;

namespace Microsoft.SemanticKernel.Connectors.AI.OpenAI.AzureSdk;

#pragma warning disable CA2208 // Instantiate argument exceptions correctly

/// <summary>
/// Base class for AI clients that provides common functionality for interacting with OpenAI services.
/// </summary>
public abstract class ClientBase
{
    private const int MaxResultsPerPrompt = 128;

    // Prevent external inheritors
    private protected ClientBase(ILoggerFactory? loggerFactory = null)
    {
        this.Logger = loggerFactory is not null ? loggerFactory.CreateLogger(this.GetType()) : NullLogger.Instance;
    }

    /// <summary>
    /// Model Id or Deployment Name
    /// </summary>
    private protected string ModelId { get; set; } = string.Empty;

    /// <summary>
    /// OpenAI / Azure OpenAI Client
    /// </summary>
    private protected abstract OpenAIClient Client { get; }

    /// <summary>
    /// Logger instance
    /// </summary>
    private protected ILogger Logger { get; set; }

    /// <summary>
    /// Instance of <see cref="Meter"/> for metrics.
    /// </summary>
    private static readonly Meter s_meter = new(typeof(ClientBase).Assembly.GetName().Name);

    /// <summary>
    /// Instance of <see cref="Counter{T}"/> to keep track of the number of prompt tokens used.
    /// </summary>
    private static readonly Counter<int> s_promptTokensCounter =
        s_meter.CreateCounter<int>(
            name: "SK.Connectors.OpenAI.PromptTokens",
            description: "Number of prompt tokens used");

    /// <summary>
    /// Instance of <see cref="Counter{T}"/> to keep track of the number of completion tokens used.
    /// </summary>
    private static readonly Counter<int> s_completionTokensCounter =
        s_meter.CreateCounter<int>(
            name: "SK.Connectors.OpenAI.CompletionTokens",
            description: "Number of completion tokens used");

    /// <summary>
    /// Instance of <see cref="Counter{T}"/> to keep track of the total number of tokens used.
    /// </summary>
    private static readonly Counter<int> s_totalTokensCounter =
        s_meter.CreateCounter<int>(
            name: "SK.Connectors.OpenAI.TotalTokens",
            description: "Total number of tokens used");

    /// <summary>
    /// Creates completions for the prompt and settings.
    /// </summary>
    /// <param name="text">The prompt to complete.</param>
    /// <param name="requestSettings">Request settings for the completion API</param>
    /// <param name="cancellationToken">The <see cref="CancellationToken"/> to monitor for cancellation requests. The default is <see cref="CancellationToken.None"/>.</param>
    /// <returns>Completions generated by the remote model</returns>
    private protected async Task<IReadOnlyList<ITextResult>> InternalGetTextResultsAsync(
        string text,
        AIRequestSettings? requestSettings,
        CancellationToken cancellationToken = default)
    {
        OpenAIRequestSettings textRequestSettings = OpenAIRequestSettings.FromRequestSettings(requestSettings, OpenAIRequestSettings.DefaultTextMaxTokens);

        ValidateMaxTokens(textRequestSettings.MaxTokens);
        var options = CreateCompletionsOptions(text, textRequestSettings);

        Response<Completions>? response = await RunRequestAsync(
            () => this.Client.GetCompletionsAsync(this.ModelId, options, cancellationToken)).ConfigureAwait(false);

        if (response is null)
        {
            throw new SKException("Text completions null response");
        }

        var responseData = response.Value;

        if (responseData.Choices.Count == 0)
        {
            throw new SKException("Text completions not found");
        }

        this.CaptureUsageDetails(responseData.Usage);

        return responseData.Choices.Select(choice => new TextResult(responseData, choice)).ToList();
    }

    /// <summary>
    /// Creates completions streams for the prompt and settings.
    /// </summary>
    /// <param name="text">The prompt to complete.</param>
    /// <param name="requestSettings">Request settings for the completion API</param>
    /// <param name="cancellationToken">The <see cref="CancellationToken"/> to monitor for cancellation requests. The default is <see cref="CancellationToken.None"/>.</param>
    /// <returns>Stream the completions generated by the remote model</returns>
    private protected async IAsyncEnumerable<TextStreamingResult> InternalGetTextStreamingResultsAsync(
        string text,
        AIRequestSettings? requestSettings,
        [EnumeratorCancellation] CancellationToken cancellationToken = default)
    {
        OpenAIRequestSettings textRequestSettings = OpenAIRequestSettings.FromRequestSettings(requestSettings, OpenAIRequestSettings.DefaultTextMaxTokens);

        ValidateMaxTokens(textRequestSettings.MaxTokens);

        var options = CreateCompletionsOptions(text, textRequestSettings);

        Response<StreamingCompletions>? response = await RunRequestAsync(
            () => this.Client.GetCompletionsStreamingAsync(this.ModelId, options, cancellationToken)).ConfigureAwait(false);

        using StreamingCompletions streamingChatCompletions = response.Value;
        await foreach (StreamingChoice choice in streamingChatCompletions.GetChoicesStreaming(cancellationToken))
        {
            yield return new TextStreamingResult(streamingChatCompletions, choice);
        }
    }

    /// <summary>
    /// Generates an embedding from the given <paramref name="data"/>.
    /// </summary>
    /// <param name="data">List of strings to generate embeddings for</param>
    /// <param name="cancellationToken">The <see cref="CancellationToken"/> to monitor for cancellation requests. The default is <see cref="CancellationToken.None"/>.</param>
    /// <returns>List of embeddings</returns>
    private protected async Task<IList<ReadOnlyMemory<float>>> InternalGetEmbeddingsAsync(
        IList<string> data,
        CancellationToken cancellationToken = default)
    {
        var result = new List<ReadOnlyMemory<float>>(data.Count);
        foreach (string text in data)
        {
            var options = new EmbeddingsOptions(text);

            Response<Embeddings>? response = await RunRequestAsync(
                () => this.Client.GetEmbeddingsAsync(this.ModelId, options, cancellationToken)).ConfigureAwait(false);

            if (response is null)
            {
                throw new SKException("Text embedding null response");
            }

            if (response.Value.Data.Count == 0)
            {
                throw new SKException("Text embedding not found");
            }

            result.Add(response.Value.Data[0].Embedding.ToArray());
        }

        return result;
    }

    /// <summary>
    /// Generate a new chat message
    /// </summary>
    /// <param name="chat">Chat history</param>
    /// <param name="context">Context containing the functions available for automatic invocation during processing and to use to invoke functions as needed; may be null to indicate no functions are available</param>
    /// <param name="requestSettings">AI request settings</param>
    /// <param name="cancellationToken">Async cancellation token</param>
    /// <returns>Generated chat message in string format</returns>
    private protected async Task<IReadOnlyList<IChatResult>> InternalGetChatResultsAsync(
        ChatHistory chat,
        SKContext? context,
        AIRequestSettings? requestSettings,
        CancellationToken cancellationToken = default)
    {
        Verify.NotNull(chat);

        OpenAIRequestSettings chatRequestSettings = OpenAIRequestSettings.FromRequestSettings(requestSettings);
        ValidateMaxTokens(chatRequestSettings.MaxTokens);

        IReadOnlyList<FunctionView>? functionViews = context?.Functions?.GetFunctionViews();
        var chatOptions = CreateChatCompletionsOptions(chatRequestSettings, chat, functionViews);

        ChatCompletions? chatCompletions;

        while (true)
        {
            // Make the call to the chat completion service.
            Response<ChatCompletions> response = await RunRequestAsync(() => this.Client.GetChatCompletionsAsync(this.ModelId, chatOptions, cancellationToken)).ConfigureAwait(false);

            chatCompletions = response.Value;
            if (chatCompletions.Choices.Count == 0)
            {
                throw new SKException("Chat completions not found");
            }

            this.CaptureUsageDetails(chatCompletions.Usage);

            var choice = chatCompletions.Choices[0];
            if (choice.FinishReason != CompletionsFinishReason.FunctionCall ||
                context?.Functions is null ||
                functionViews is not { Count: > 0 })
            {
                // The response is not a function call, or it's a function call even though no functions were supplied (they might have
                // been in the AIRequestSettings but that's not something we can act on here). The result of this GetChatCompletionsAsync
                // call is the final result.
                break;
            }

            // Handle function invocation request.

            // Find the ISKFunction to invoke.
            OpenAIFunctionResponse functionCall = OpenAIFunctionResponse.FromFunctionCall(choice.Message.FunctionCall);
            if (!context.Functions.TryGetFunction(functionCall.PluginName, functionCall.FunctionName, out ISKFunction? function))
            {
                // The requested function doesn't exist.
                throw new SKException("Requested function not found");
            }

            // Set the arguments to the function into the context in order to invoke it.
            foreach (KeyValuePair<string, object> parameter in functionCall.Parameters)
            {
                context.Variables.Set(parameter.Key, parameter.Value?.ToString() ?? "");
            }

            // Invoke the function
            FunctionResult functionResult = await function.InvokeAsync(context, requestSettings, cancellationToken).ConfigureAwait(false);
            chatOptions.Messages.Add(new ChatMessage(ChatRole.Function, functionResult.GetValue<object>()?.ToString() ?? "") { Name = choice.Message.FunctionCall.Name });
        }

        return chatCompletions.Choices.Select(chatChoice => new ChatResult(chatCompletions, chatChoice)).ToList();
    }

    /// <summary>
    /// Generate a new chat message stream
    /// </summary>
    /// <param name="chat">Chat history</param>
    /// <param name="context">Context containing the functions available for automatic invocation during processing and to use to invoke functions as needed; may be null to indicate no functions are available</param>
    /// <param name="requestSettings">AI request settings</param>
    /// <param name="cancellationToken">Async cancellation token</param>
    /// <returns>Streaming of generated chat message in string format</returns>
    private protected async IAsyncEnumerable<IChatStreamingResult> InternalGetChatStreamingResultsAsync(
        IEnumerable<ChatMessageBase> chat,
        SKContext? context,
        AIRequestSettings? requestSettings,
        [EnumeratorCancellation] CancellationToken cancellationToken = default)
    {
        Verify.NotNull(chat);

        OpenAIRequestSettings chatRequestSettings = OpenAIRequestSettings.FromRequestSettings(requestSettings);
        ValidateMaxTokens(chatRequestSettings.MaxTokens);

        IReadOnlyList<FunctionView>? functionViews = context?.Functions?.GetFunctionViews();
        var chatOptions = CreateChatCompletionsOptions(chatRequestSettings, chat, functionViews);

        // Make the call to the chat completion service.
        Response<StreamingChatCompletions> response = await RunRequestAsync(() => this.Client.GetChatCompletionsStreamingAsync(this.ModelId, chatOptions, cancellationToken)).ConfigureAwait(false);

        // Process all of the choices in the response.
        using StreamingChatCompletions streamingChatCompletions = response.Value;
        await foreach (StreamingChatChoice choice in streamingChatCompletions.GetChoicesStreaming(cancellationToken).ConfigureAwait(false))
        {
            // Ensure each fork has its own chat options so that added messages don't interact with each other.
            // We also ensure the chat count is reset to 1 if it had been higher, to avoid forking out again
            // when sending responses from function calls.
            var newOptions = Clone(chatOptions);
            newOptions.ChoiceCount = 1;
            yield return new FunctionsChatStreamingResult(this, streamingChatCompletions, choice, functionViews, newOptions, context, requestSettings);
        }
    }

    private sealed class FunctionsChatStreamingResult : IChatStreamingResult
    {
        private readonly ClientBase _client;
        private readonly StreamingChatChoice _choice;
        private readonly IReadOnlyList<FunctionView>? _functionViews;
        private readonly ChatCompletionsOptions _chatOptions;
        private readonly SKContext? _context;
        private readonly AIRequestSettings? _requestSettings;

        public FunctionsChatStreamingResult(
            ClientBase client,
            StreamingChatCompletions resultData,
            StreamingChatChoice choice,
            IReadOnlyList<FunctionView>? functionViews,
            ChatCompletionsOptions chatOptions,
            SKContext? context,
            AIRequestSettings? requestSettings)
        {
            this._client = client;
            this.ModelResult = new ModelResult(resultData);
            this._choice = choice;
            this._functionViews = functionViews;
            this._chatOptions = chatOptions;
            this._context = context;
            this._requestSettings = requestSettings;
        }

        public ModelResult ModelResult { get; }

        public async Task<ChatMessageBase> GetChatMessageAsync(CancellationToken cancellationToken = default)
        {
            ChatMessageBase? last = null;
            await foreach (ChatMessageBase message in this.GetStreamingChatMessageAsync(cancellationToken).ConfigureAwait(false))
            {
                last = message;
            }

            return last ?? throw new SKException("Unable to get chat message from stream");
        }

        public async IAsyncEnumerable<ChatMessageBase> GetStreamingChatMessageAsync([EnumeratorCancellation] CancellationToken cancellationToken)
        {
            Response<StreamingChatCompletions>? response = null;
            StreamingChatChoice choice = this._choice;
            string? functionName;
            StringBuilder? functionArguments = null;

            do
            {
                functionName = null;
                functionArguments?.Clear();

                StreamingChatCompletions? streamingChatCompletions = response?.Value;
                IAsyncEnumerator<StreamingChatChoice>? choices = null;
                try
                {
                    // Stream the new chat if we have a new response to handle. `response`, and thus `streamingChatCompletions`,
                    // will be null on first iteration.
                    if (streamingChatCompletions is not null)
                    {
                        choices = streamingChatCompletions.GetChoicesStreaming(cancellationToken).GetAsyncEnumerator(cancellationToken);
                        if (!await choices.MoveNextAsync().ConfigureAwait(false))
                        {
                            break;
                        }

                        choice = choices.Current;
                    }

                    // Enumerate this choice
                    IAsyncEnumerator<ChatMessage> messagesEnumerator = choice.GetMessageStreaming(cancellationToken).GetAsyncEnumerator(cancellationToken);
                    await using var _ = messagesEnumerator.ConfigureAwait(false);
                    while (await messagesEnumerator.MoveNextAsync().ConfigureAwait(false))
                    {
                        ChatMessage? message = messagesEnumerator.Current;

                        // Not a function call; yield the message. We yield each message individually as its own
                        // result since we don't know if there will be a function call following it and we
                        // don't want to buffer messages.
                        if (message.FunctionCall is null)
                        {
                            yield return new SKChatMessage(message);
                        }
                        else
                        {
                            // Function call: gather up all parts of it to grab the function name and arguments.
                            // It can be spread across many messages, and nothing meaningful should come after
                            // the function's parts in the choice, so we can ignore anything there.
                            do
                            {
                                // Aggregate the current message's function information.
                                message = messagesEnumerator.Current;
                                functionName ??= message.FunctionCall?.Name;
                                if (message.FunctionCall?.Arguments is not null)
                                {
                                    (functionArguments ??= new()).Append(message.FunctionCall.Arguments);
                                }
                            }
                            while (await messagesEnumerator.MoveNextAsync().ConfigureAwait(false));
                            break;
                        }
                    }
                }
                finally
                {
                    if (choices is not null)
                    {
                        await choices.DisposeAsync().ConfigureAwait(false);
                    }

                    streamingChatCompletions?.Dispose();
                }

                // If there's no function to invoke, we're done.
                if (functionName is null)
                {
                    break;
                }

                // Find the ISKFunction to invoke.
                OpenAIFunctionResponse functionCall = OpenAIFunctionResponse.FromFunctionCall(new FunctionCall(functionName, functionArguments?.ToString() ?? ""));
                if (this._context is null || !this._context.Functions.TryGetFunction(functionCall.PluginName, functionCall.FunctionName, out ISKFunction? function))
                {
                    // The requested function doesn't exist.
                    throw new SKException("Requested function not found");
                }

                // Set the arguments to the function into the context in order to invoke it.
                foreach (KeyValuePair<string, object> parameter in functionCall.Parameters)
                {
                    this._context.Variables.Set(parameter.Key, parameter.Value?.ToString() ?? "");
                }

                // Invoke the function, and store the result into the chat options for the next go around.
                FunctionResult functionResult = await function.InvokeAsync(this._context, this._requestSettings, cancellationToken).ConfigureAwait(false);
                this._chatOptions.Messages.Add(new ChatMessage(ChatRole.Function, functionResult.GetValue<object>()?.ToString() ?? "") { Name = function.Name });

                // Make the call to the chat completion service to resubmit with the function result.
                response = await RunRequestAsync(() => this._client.Client.GetChatCompletionsStreamingAsync(this._client.ModelId, this._chatOptions, cancellationToken)).ConfigureAwait(false);
            }
            while (true);
        }
    }

    /// <summary>
    /// Create a new empty chat instance
    /// </summary>
    /// <param name="instructions">Optional chat instructions for the AI service</param>
    /// <returns>Chat object</returns>
    private protected static OpenAIChatHistory InternalCreateNewChat(string? instructions = null)
    {
        return new OpenAIChatHistory(instructions);
    }

    private protected async Task<IReadOnlyList<ITextResult>> InternalGetChatResultsAsTextAsync(
        string text,
        AIRequestSettings? requestSettings,
        CancellationToken cancellationToken = default)
    {
        ChatHistory chat = PrepareChatHistory(text, requestSettings, out OpenAIRequestSettings chatSettings);

        return (await this.InternalGetChatResultsAsync(chat, null, chatSettings, cancellationToken).ConfigureAwait(false))
            .OfType<ITextResult>()
            .ToList();
    }

    private protected async IAsyncEnumerable<ITextStreamingResult> InternalGetChatStreamingResultsAsTextAsync(
        string text,
        AIRequestSettings? requestSettings,
        [EnumeratorCancellation] CancellationToken cancellationToken = default)
    {
        ChatHistory chat = PrepareChatHistory(text, requestSettings, out OpenAIRequestSettings chatSettings);

        IAsyncEnumerable<IChatStreamingResult> chatCompletionStreamingResults = this.InternalGetChatStreamingResultsAsync(chat, null, chatSettings, cancellationToken);
        await foreach (var chatCompletionStreamingResult in chatCompletionStreamingResults)
        {
            yield return (ITextStreamingResult)chatCompletionStreamingResult;
        }
    }

    private static OpenAIChatHistory PrepareChatHistory(string text, AIRequestSettings? requestSettings, out OpenAIRequestSettings settings)
    {
        settings = OpenAIRequestSettings.FromRequestSettings(requestSettings);
        var chat = InternalCreateNewChat(settings.ChatSystemPrompt);
        chat.AddUserMessage(text);
        return chat;
    }

    private static CompletionsOptions CreateCompletionsOptions(string text, OpenAIRequestSettings requestSettings)
    {
        if (requestSettings.ResultsPerPrompt is < 1 or > MaxResultsPerPrompt)
        {
            throw new ArgumentOutOfRangeException($"{nameof(requestSettings)}.{nameof(requestSettings.ResultsPerPrompt)}", requestSettings.ResultsPerPrompt, $"The value must be in range between 1 and {MaxResultsPerPrompt}, inclusive.");
        }

        var options = new CompletionsOptions
        {
            Prompts = { text.NormalizeLineEndings() },
            MaxTokens = requestSettings.MaxTokens,
            Temperature = (float?)requestSettings.Temperature,
            NucleusSamplingFactor = (float?)requestSettings.TopP,
            FrequencyPenalty = (float?)requestSettings.FrequencyPenalty,
            PresencePenalty = (float?)requestSettings.PresencePenalty,
            Echo = false,
            ChoicesPerPrompt = requestSettings.ResultsPerPrompt,
            GenerationSampleCount = requestSettings.ResultsPerPrompt,
            LogProbabilityCount = null,
            User = null,
        };

        foreach (var keyValue in requestSettings.TokenSelectionBiases)
        {
            options.TokenSelectionBiases.Add(keyValue.Key, keyValue.Value);
        }

        if (requestSettings.StopSequences is { Count: > 0 })
        {
            foreach (var s in requestSettings.StopSequences)
            {
                options.StopSequences.Add(s);
            }
        }

        return options;
    }

    private static ChatCompletionsOptions CreateChatCompletionsOptions(
        OpenAIRequestSettings requestSettings,
        IEnumerable<ChatMessageBase> chatHistory,
        IReadOnlyList<FunctionView>? functions)
    {
        if (requestSettings.ResultsPerPrompt is < 1 or > MaxResultsPerPrompt)
        {
            throw new ArgumentOutOfRangeException($"{nameof(requestSettings)}.{nameof(requestSettings.ResultsPerPrompt)}", requestSettings.ResultsPerPrompt, $"The value must be in range between 1 and {MaxResultsPerPrompt}, inclusive.");
        }

        var options = new ChatCompletionsOptions
        {
            MaxTokens = requestSettings.MaxTokens,
            Temperature = (float?)requestSettings.Temperature,
            NucleusSamplingFactor = (float?)requestSettings.TopP,
            FrequencyPenalty = (float?)requestSettings.FrequencyPenalty,
            PresencePenalty = (float?)requestSettings.PresencePenalty,
            ChoiceCount = requestSettings.ResultsPerPrompt,
        };

        // Supplied functions take precedence over request settings functions
        if (functions is not null)
        {
            options.FunctionCall = FunctionDefinition.Auto;
            options.Functions = functions.Select(f => f.ToOpenAIFunction().ToFunctionDefinition()).ToList();
        }
        else if (requestSettings.Functions is not null)
        {
            if (requestSettings.FunctionCall == OpenAIRequestSettings.FunctionCallAuto)
            {
                options.FunctionCall = FunctionDefinition.Auto;
                options.Functions = requestSettings.Functions.Select(f => f.ToFunctionDefinition()).ToList();
            }
            else if (requestSettings.FunctionCall != OpenAIRequestSettings.FunctionCallNone
                    && !requestSettings.FunctionCall.IsNullOrEmpty())
            {
                var filteredFunctions = requestSettings.Functions
                    .Where(f => f.FullyQualifiedName.Equals(requestSettings.FunctionCall, StringComparison.OrdinalIgnoreCase))
                    .ToList();

                OpenAIFunction? function = filteredFunctions.FirstOrDefault();
                if (function is not null)
                {
                    options.FunctionCall = function.ToFunctionDefinition();
                    options.Functions = filteredFunctions.Select(f => f.ToFunctionDefinition()).ToList();
                }
            }
        }

        foreach (var keyValue in requestSettings.TokenSelectionBiases)
        {
            options.TokenSelectionBiases.Add(keyValue.Key, keyValue.Value);
        }

        if (requestSettings.StopSequences is { Count: > 0 })
        {
            foreach (var s in requestSettings.StopSequences)
            {
                options.StopSequences.Add(s);
            }
        }

        foreach (var message in chatHistory)
        {
            var azureMessage = new ChatMessage(GetValidChatRole(message.Role), message.Content);

            if (message.AdditionalContext?.TryGetValue("Name", out string? name) is true)
            {
                azureMessage.Name = name;
            }

            options.Messages.Add(azureMessage);
        }

        return options;
    }

    private static ChatCompletionsOptions Clone(ChatCompletionsOptions original)
    {
        var newOptions = new ChatCompletionsOptions(original.Messages)
        {
            AzureExtensionsOptions = original.AzureExtensionsOptions,
            ChoiceCount = original.ChoiceCount,
            FrequencyPenalty = original.FrequencyPenalty,
            FunctionCall = original.FunctionCall,
            Functions = original.Functions,
            MaxTokens = original.MaxTokens,
            NucleusSamplingFactor = original.NucleusSamplingFactor,
            PresencePenalty = original.PresencePenalty,
            Temperature = original.Temperature,
            User = original.User,
        };

        foreach (var s in original.StopSequences)
        {
            newOptions.StopSequences.Add(s);
        }

        foreach (var t in original.TokenSelectionBiases)
        {
            newOptions.TokenSelectionBiases.Add(t);
        }

        return newOptions;
    }

    private static ChatRole GetValidChatRole(AuthorRole role)
    {
        var validRole = new ChatRole(role.Label);

        if (validRole != ChatRole.User &&
            validRole != ChatRole.System &&
            validRole != ChatRole.Assistant &&
            validRole != ChatRole.Function)
        {
            throw new ArgumentException($"Invalid chat message author role: {role}");
        }

        return validRole;
    }

    private static void ValidateMaxTokens(int? maxTokens)
    {
        if (maxTokens.HasValue && maxTokens < 1)
        {
            throw new SKException($"MaxTokens {maxTokens} is not valid, the value must be greater than zero");
        }
    }

    private static async Task<T> RunRequestAsync<T>(Func<Task<T>> request)
    {
        try
        {
            return await request().ConfigureAwait(false);
        }
        catch (RequestFailedException e)
        {
            throw e.ToHttpOperationException();
        }
    }

    /// <summary>
    /// Captures usage details, including token information.
    /// </summary>
    /// <param name="usage">Instance of <see cref="CompletionsUsage"/> with usage details.</param>
    private void CaptureUsageDetails(CompletionsUsage usage)
    {
        this.Logger.LogInformation(
            "Prompt tokens: {PromptTokens}. Completion tokens: {CompletionTokens}. Total tokens: {TotalTokens}.",
            usage.PromptTokens, usage.CompletionTokens, usage.TotalTokens);

        s_promptTokensCounter.Add(usage.PromptTokens);
        s_completionTokensCounter.Add(usage.CompletionTokens);
        s_totalTokensCounter.Add(usage.TotalTokens);
    }
}
